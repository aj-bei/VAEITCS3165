{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "52c7d437",
      "metadata": {
        "id": "52c7d437"
      },
      "source": [
        "# Variational Autoencoder Experiments\n",
        "\n",
        "## By A.J. Beiza for ITCS 3156 Final Project "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "693b74d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "693b74d6",
        "outputId": "42cb8589-58eb-44ff-a53a-aaa097ea805b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x28d81702180>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Union, Dict\n",
        "from torchvision.datasets import MNIST\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from datetime import datetime\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "working_dir = os.getcwd()\n",
        "date = datetime.strftime(datetime.now(), \"%Y-%m-%d-%H%M%S\")\n",
        "\n",
        "torch.manual_seed(7262005)  # bday\n",
        "np.random.seed(7262005)\n",
        "plt.ioff()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BK2GCcc_v0T5",
      "metadata": {
        "id": "BK2GCcc_v0T5"
      },
      "source": [
        "# Download MNIST Dataset from PyTorch for use with the simple linear VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "an-PcjG9v0zL",
      "metadata": {
        "id": "an-PcjG9v0zL"
      },
      "outputs": [],
      "source": [
        "mnist_download_dir = os.path.join(working_dir, \"data\", \"MNIST_downloaded\")\n",
        "\n",
        "os.makedirs(os.path.join(working_dir, \"data\"), exist_ok=True)\n",
        "os.makedirs(mnist_download_dir, exist_ok=True)\n",
        "\n",
        "if len(os.listdir(mnist_download_dir)) == 0:\n",
        "    download = True\n",
        "else:\n",
        "    download = False\n",
        "\n",
        "mnist_dataset = MNIST(\n",
        "    root=mnist_download_dir,\n",
        "    train=True,\n",
        "    download=download,\n",
        "    transform=T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Lambda(lambda x: x.numpy())\n",
        "    ])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "z9-wsdFv56FG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "z9-wsdFv56FG",
        "outputId": "564beca1-5f22-4022-8448-457ec071b28e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def visualize_rnd_imgs(dataset: Dataset, nrow:int, ncol:int, img_idx:int, greyscale:bool, reshape:callable):\n",
        "\n",
        "  \"\"\"\n",
        "  dataset: A PyTorch dataset with the len() and __get_item__() methods implemented\n",
        "  nrow: The number of rows in the figure\n",
        "  ncol: The number of columns in the figure\n",
        "  img_idx: The index of the image/tensor/array in the tuple returned by dataset.__getitem__()\n",
        "\n",
        "  Returns:\n",
        "  fig: Matplotlib figure\n",
        "  axs: list of matplotlib axes\n",
        "  \"\"\"\n",
        "\n",
        "  rnd_idxs = np.random.choice(np.arange(len(dataset)), size=nrow*ncol)\n",
        "  fig, axs = plt.subplots(nrow, ncol, figsize=(10, 5))\n",
        "  flat_axs = axs.flatten()\n",
        "  for i, idx in enumerate(rnd_idxs):\n",
        "      img = dataset[idx][img_idx]\n",
        "\n",
        "      if greyscale:\n",
        "        flat_axs[i].imshow(reshape(img), cmap=\"Greys_r\")\n",
        "      else:\n",
        "        flat_axs[i].imshow(reshape(img))\n",
        "\n",
        "      flat_axs[i].axis(\"off\")\n",
        "\n",
        "  return fig, axs\n",
        "\n",
        "fig, _ = visualize_rnd_imgs(mnist_dataset, nrow=2, ncol=5, img_idx=0, greyscale=True, reshape= lambda x: x.reshape(28, 28))\n",
        "fig.suptitle(\"Random Images from MNIST Dataset\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UIhFNdWGrSdp",
      "metadata": {
        "id": "UIhFNdWGrSdp"
      },
      "source": [
        "# Download Labeled Faces in the Wild (LFW) Dataset for use with the Convolutional VAE (CVAE)\n",
        "\n",
        "To explore the full potential of the VAE architecture, a more complex dataset with more complex layers can be used. Convolutional layers are better equipped to understand and model images because they are not treating pixels as indivdual features and can model the spatial relationship of pictures better.\n",
        "\n",
        "This code does take a very long time to run (~30-40 minutes) if the dataset is not downloaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yZQPV2UHQGur",
      "metadata": {
        "id": "yZQPV2UHQGur"
      },
      "outputs": [],
      "source": [
        "lfw_download_dir = os.path.join(working_dir, \"data\", \"LFW_downloaded\")\n",
        "lfw_out_dir = os.path.join(working_dir, \"data\", \"LFW\")\n",
        "\n",
        "os.makedirs(os.path.join(working_dir, \"data\"), exist_ok=True)\n",
        "os.makedirs(lfw_download_dir, exist_ok=True)\n",
        "os.makedirs(lfw_out_dir, exist_ok=True)\n",
        "\n",
        "# download dataset if needed\n",
        "if len(os.listdir(lfw_out_dir)) == 0:\n",
        "\n",
        "    lfw = fetch_lfw_people(\n",
        "        data_home=lfw_download_dir,\n",
        "        funneled=True,  # cleaned version of the dataset\n",
        "        download_if_missing=True\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    Walk through downloaded folder structure which makes\n",
        "    a folder for each person and flatten the folders to one\n",
        "    folder with all images (since labels are not needed for\n",
        "    this unsupervised task\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    for root, dirs, files in os.walk(lfw_download_dir):\n",
        "        for f in files:\n",
        "            if f.endswith(\".jpg\"):\n",
        "                person_name = \"_\".join(f.split(\"_\")[:2])\n",
        "                # new file name with 0 padding\n",
        "                new_name = f\"lfw_{i:06d}_{person_name}.jpg\"\n",
        "                shutil.copy(\n",
        "                    os.path.join(root, f),\n",
        "                    os.path.join(lfw_out_dir, new_name)\n",
        "                )\n",
        "                i += 1\n",
        "    print(f\"All files moved to {lfw_out_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oLA5iMkY4g4x",
      "metadata": {
        "id": "oLA5iMkY4g4x"
      },
      "source": [
        "Use the folder path to create a class inheriting from Dataset to allow for easy integration with other PyTorch classes, such as DataLoader for Mini batch training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MNmGtNL01uhh",
      "metadata": {
        "id": "MNmGtNL01uhh"
      },
      "outputs": [],
      "source": [
        "class LFWDataset(Dataset):\n",
        "\n",
        "    def __init__(self, folder_path: str, transform=None, greyscale=False):\n",
        "        assert os.path.exists(folder_path), f\"Folder Path: {folder_path} does not exist\"\n",
        "        self.path = folder_path\n",
        "        self.img_nms = os.listdir(folder_path)\n",
        "        self.transform = transform\n",
        "        self.color = \"L\" if greyscale else \"RGB\"\n",
        "\n",
        "    def __len__(self)->int:\n",
        "        return len(self.img_nms)\n",
        "\n",
        "    def __getitem__(self, idx)->Union[torch.Tensor, np.ndarray, Image]:\n",
        "        f_name = os.path.join(self.path, self.img_nms[idx])\n",
        "        person_name = \" \".join(f_name.split(\"_\")[-2:])\n",
        "        img = Image.open(f_name).convert(self.color)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        img = img.to(torch.float32)\n",
        "        img = torch.clamp(img, min=0.0, max=1.0)\n",
        "        return (img, person_name)\n",
        "\n",
        "lfw_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Resize((64, 64))\n",
        "])\n",
        "\n",
        "lfw_dataset = LFWDataset(folder_path=lfw_out_dir, transform=lfw_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H_f57S_s4r5T",
      "metadata": {
        "id": "H_f57S_s4r5T"
      },
      "source": [
        "Visualize random samples from the LFW Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TxO-vysx4vUe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "TxO-vysx4vUe",
        "outputId": "a784b7c6-4aca-41ab-afce-ad02fbfb340f"
      },
      "outputs": [],
      "source": [
        "fig, _ = visualize_rnd_imgs(\n",
        "    dataset=lfw_dataset,\n",
        "    nrow=3,\n",
        "    ncol=5,\n",
        "    img_idx=0,\n",
        "    greyscale=False,\n",
        "    reshape= lambda x: x.permute(1, 2, 0)\n",
        ")\n",
        "fig.suptitle(\"Random Images from LFW Dataset\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c1f331de",
      "metadata": {
        "id": "c1f331de"
      },
      "outputs": [],
      "source": [
        "class VariationalAutoencoder:\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            lr:float,\n",
        "            l1_dim:int,\n",
        "            l2_dim:int,\n",
        "            latent_dim:int,\n",
        "            l4_dim:int,\n",
        "            l5_dim:int,\n",
        "            mse:bool  # uses cross entropy loss for reconstruction loss if False\n",
        "        ):\n",
        "\n",
        "        self.lr = lr\n",
        "        self.l1_dim = l1_dim\n",
        "        self.l2_dim = l2_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.l4_dim = l4_dim\n",
        "        self.l5_dim = l5_dim\n",
        "        self.beta = 0\n",
        "        self.mse = mse\n",
        "        self.params_init = False  # used to avoid initiaizing params >1x\n",
        "        self.test_losses = {\"Recon Loss\": [], \"KL Loss\": [], \"ELBO Loss\": []}\n",
        "        self.train_losses = {\"Recon Loss\": [], \"KL Loss\": [], \"ELBO Loss\": []}\n",
        "\n",
        "\n",
        "    def he_init(self, fan_in:int, shape:Tuple)->np.ndarray:\n",
        "\n",
        "        std = np.sqrt(2.0 / fan_in)\n",
        "        return np.random.normal(loc=0, scale=std, size=shape)\n",
        "\n",
        "\n",
        "    def init_params(self, dim:int):\n",
        "\n",
        "        # first hidden layer weights + bias\n",
        "        self.W1_enc = self.he_init(fan_in=dim, shape=(dim, self.l1_dim))  # dim x l1_dim\n",
        "        self.b1_enc = np.zeros((self.l1_dim, ))  # l1_dim x 1\n",
        "\n",
        "        # second hidden layer weights + bias\n",
        "        self.W2_enc = self.he_init(fan_in=self.l1_dim, shape=(self.l1_dim, self.l2_dim))  # l1_dim x l2_dim\n",
        "        self.b2_enc = np.zeros((self.l2_dim, ))  # l2_dim x 1\n",
        "        # mu of latent space layer weights + bias\n",
        "        # self.Wmu = np.random.normal(loc=0, scale=0, size=(self.l2_dim, self.latent_dim))\n",
        "        self.Wmu = self.he_init(fan_in=self.l2_dim, shape=(self.l2_dim, self.latent_dim))\n",
        "        self.bmu = np.zeros((self.latent_dim, ))  # latent_dim x 1\n",
        "\n",
        "        # std of latent space layer weights + bias\n",
        "        # self.Wstd = np.random.normal(loc=0, scale=0, size=(self.l2_dim, self.latent_dim))\n",
        "        self.Wstd = self.he_init(fan_in=self.l2_dim, shape=(self.l2_dim, self.latent_dim))\n",
        "        self.bstd = np.zeros((self.latent_dim, ))  # latent_dim x 1\n",
        "\n",
        "        # 4th hidden layer weights + bias (first hidden layer of decoder)\n",
        "        self.W1_dec = self.he_init(fan_in=self.latent_dim, shape=(self.latent_dim, self.l4_dim))\n",
        "        self.b1_dec = np.zeros((self.l4_dim, ))\n",
        "\n",
        "        # 5th hidden layer weights + bias\n",
        "        self.W2_dec = self.he_init(fan_in=self.l4_dim, shape=(self.l4_dim, self.l5_dim))\n",
        "        self.b2_dec = np.zeros((self.l5_dim, ))\n",
        "\n",
        "        # output layer weights + bias\n",
        "        self.W3_dec = self.he_init(fan_in=self.l5_dim, shape=(self.l5_dim, dim))\n",
        "        self.b3_dec = np.zeros((dim, ))\n",
        "\n",
        "        # init gradients\n",
        "        self.W1_enc_d = np.zeros_like(self.W1_enc)\n",
        "        self.b1_enc_d = np.zeros_like(self.b1_enc)\n",
        "        self.W2_enc_d = np.zeros_like(self.W2_enc)\n",
        "        self.b2_enc_d = np.zeros_like(self.b2_enc)\n",
        "        self.Wmu_d = np.zeros_like(self.Wmu)\n",
        "        self.bmu_d = np.zeros_like(self.bmu)\n",
        "        self.Wstd_d = np.zeros_like(self.Wstd)\n",
        "        self.bstd_d = np.zeros_like(self.bstd)\n",
        "        self.W1_dec_d = np.zeros_like(self.W1_dec)\n",
        "        self.b1_dec_d = np.zeros_like(self.b1_dec)\n",
        "        self.W2_dec_d = np.zeros_like(self.W2_dec)\n",
        "        self.b2_dec_d = np.zeros_like(self.b2_dec)\n",
        "        self.W3_dec_d = np.zeros_like(self.W3_dec)\n",
        "        self.b3_dec_d = np.zeros_like(self.b3_dec)\n",
        "        self.params_init = True  # mark params as initialized\n",
        "\n",
        "    def relu(self, X, derivative=False):\n",
        "\n",
        "        if not derivative:\n",
        "            return (X > 0) * X\n",
        "        return (X > 0) * 1\n",
        "\n",
        "\n",
        "    def lrelu(self, x, alpha=0.1, derivative=False):\n",
        "\n",
        "        if not derivative:\n",
        "          return np.where(x > 0, x, alpha * x)\n",
        "        drv = np.ones_like(x)\n",
        "        drv[x < 0] = alpha\n",
        "        return drv\n",
        "\n",
        "\n",
        "    def encoder(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "\n",
        "        # check if parameters have been init. yet\n",
        "        if not self.params_init:\n",
        "            self.init_params(X.shape[1])\n",
        "\n",
        "        self.z1_enc = X @ self.W1_enc + self.b1_enc\n",
        "        self.a1_enc = self.lrelu(self.z1_enc)\n",
        "\n",
        "        self.z2_enc = self.a1_enc @ self.W2_enc + self.b2_enc\n",
        "        self.a2_enc = self.lrelu(self.z2_enc)\n",
        "\n",
        "        self.mu = self.a2_enc @ self.Wmu + self.bmu\n",
        "        self.logvar = self.a2_enc @ self.Wstd + self.bstd\n",
        "\n",
        "        return self.mu, self.logvar\n",
        "\n",
        "    def reparametrize(self, mu: np.ndarray, logvar: np.ndarray, length: int)->np.ndarray:\n",
        "\n",
        "        self.eps = np.random.standard_normal(size=(length, self.latent_dim))\n",
        "        return mu + np.exp(0.5*logvar) * self.eps\n",
        "\n",
        "    def decoder(self, Z:np.ndarray) -> np.ndarray:\n",
        "\n",
        "        self.z1_dec = Z @ self.W1_dec + self.b1_dec\n",
        "        self.a1_dec = self.lrelu(self.z1_dec)\n",
        "\n",
        "        self.z2_dec = self.a1_dec @ self.W2_dec + self.b2_dec\n",
        "        self.a2_dec = self.lrelu(self.z2_dec)\n",
        "\n",
        "        mu_x = self.a2_dec @ self.W3_dec + self.b3_dec\n",
        "\n",
        "        return mu_x  # these are interpreted as logits if self.mse is false\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
        "\n",
        "        # check if parameters have been init. yet\n",
        "        if not self.params_init:\n",
        "            self.init_params(X.shape[1])\n",
        "\n",
        "        mu, logvar = self.encoder(X)\n",
        "        self.z = self.reparametrize(mu=mu, logvar=logvar, length=X.shape[0])\n",
        "        mu_x = self.decoder(self.z)\n",
        "        return mu_x, mu, logvar\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"\n",
        "        Numerically stable way to compute sigmoid. This method was written by\n",
        "        ChatGPT after I discussed numerical stability issues I was having while\n",
        "        training the VAE.\n",
        "        \"\"\"\n",
        "        pos = x >= 0\n",
        "        z = np.zeros_like(x)\n",
        "        z[pos]  = 1.0 / (1.0 + np.exp(-x[pos]))\n",
        "        z[~pos] = np.exp(x[~pos]) / (1.0 + np.exp(x[~pos]))\n",
        "        return z\n",
        "\n",
        "    def bce_with_logits(self, logits, targets):\n",
        "        \"\"\"\n",
        "        Numerically stable way to calculate binary cross entropy loss using logits.\n",
        "        This method was written by ChatGPT after I discussed numerical stability\n",
        "        issues I was having while training the VAE.\n",
        "        \"\"\"\n",
        "        x, y = logits, targets\n",
        "        loss_px = np.maximum(x, 0) - x*y + np.log1p(np.exp(-np.abs(x)))\n",
        "        return np.mean(np.sum(loss_px, axis=1))\n",
        "\n",
        "    def loss(self, X:np.ndarray, mu_x:np.ndarray, logvar:np.ndarray, mu:np.ndarray)->float:\n",
        "\n",
        "        kl_div = np.mean(-0.5 * np.sum(1 + logvar - mu**2 - np.exp(logvar), axis=1))\n",
        "        if not self.mse:\n",
        "            recon_loss = self.bce_with_logits(mu_x, X)\n",
        "        else:\n",
        "            recon_loss = np.mean(np.sum((mu_x-X)**2, axis=1))\n",
        "        elbo = recon_loss + self.beta * kl_div\n",
        "        return recon_loss, kl_div, elbo\n",
        "\n",
        "    def backwards(self, X, mu_x, logvar, mu)->None:\n",
        "\n",
        "\n",
        "        dnm = len(X)**(-1)  # denominator to average across batch\n",
        "\n",
        "        ##############################################\n",
        "        ##### DECODER GRADS #########################\n",
        "        ##############################################\n",
        "\n",
        "        # drl refers to deriviative of Reconstruction loss. KL loss can be ignored for decoder since\n",
        "        # it does not depend on decoder weights/biases\n",
        "\n",
        "        if not self.mse:\n",
        "            p = self.sigmoid(mu_x)\n",
        "            drl_dmu_x = (p - X)\n",
        "        else:\n",
        "            drl_dmu_x = 2*(mu_x-X)\n",
        "\n",
        "        self.W3_dec_d   = dnm * self.a2_dec.T @ drl_dmu_x\n",
        "        self.b3_dec_d   = dnm * np.sum(drl_dmu_x, axis=0)\n",
        "\n",
        "        dmux_da2        = self.W3_dec.T\n",
        "        da2_dz2         = self.lrelu(self.z2_dec, derivative=True)\n",
        "        drl_dz2         = drl_dmu_x @ dmux_da2 * da2_dz2\n",
        "        self.W2_dec_d   = dnm * self.a1_dec.T @ (drl_dz2)\n",
        "        self.b2_dec_d   = dnm * np.sum(drl_dz2, axis=0)\n",
        "\n",
        "        dz2_da1         = self.W2_dec.T\n",
        "        da1_dz1         = self.lrelu(self.z1_dec, derivative=True)\n",
        "        drl_dz1         = drl_dz2 @ dz2_da1 * da1_dz1\n",
        "        self.W1_dec_d   = dnm * self.z.T @ (drl_dz1)\n",
        "        self.b1_dec_d   = dnm * np.sum(drl_dz1, axis=0)\n",
        "\n",
        "        ##############################################\n",
        "        ##### ENCODER GRADS  #########################\n",
        "        ##############################################\n",
        "\n",
        "        dz1_dz      = self.W1_dec.T\n",
        "        drl_dz      = drl_dz1 @ dz1_dz\n",
        "        dkl_dmu     = mu\n",
        "        drl_dmu     = drl_dz * 1  # deriv of z wrt mu is 1\n",
        "        dl_dmu      = drl_dmu + self.beta * dkl_dmu\n",
        "\n",
        "        dkl_dlv     = -0.5 * (1-np.exp(logvar))  # derivative of KL divergence wrt logvar\n",
        "        drl_dlv     = drl_dz * 0.5 * np.exp(0.5 * logvar) * self.eps  # right side is dz/dlogvar\n",
        "        dl_dlv      = drl_dlv + self.beta * dkl_dlv\n",
        "\n",
        "        self.Wmu_d  = dnm * self.a2_enc.T @ dl_dmu\n",
        "        self.bmu_d  = dnm * np.sum(dl_dmu, axis=0)\n",
        "        self.Wstd_d = dnm * self.a2_enc.T @ dl_dlv\n",
        "        self.bstd_d = dnm * np.sum(dl_dlv, axis=0)\n",
        "\n",
        "        # here, the deriv. of loss wrt logvar and wrt mu branches can be combined\n",
        "        dl_da2          = dl_dmu @ self.Wmu.T + dl_dlv @ self.Wstd.T\n",
        "        da2_dz2_enc     = self.lrelu(self.z2_enc, derivative=True)\n",
        "        dl_dz2          = dl_da2 * da2_dz2_enc\n",
        "\n",
        "        self.W2_enc_d   = dnm * self.a1_enc.T @ dl_dz2\n",
        "        self.b2_enc_d   = dnm * np.sum(dl_dz2, axis=0)\n",
        "\n",
        "        dz2_da1_enc     = self.W2_enc.T\n",
        "        da1_dz1_enc     = self.lrelu(self.z1_enc, derivative=True)\n",
        "        dl_dz1          = dl_dz2 @ dz2_da1_enc * da1_dz1_enc\n",
        "\n",
        "        self.W1_enc_d   = dnm * X.T @ dl_dz1\n",
        "        self.b1_enc_d   = dnm * np.sum(dl_dz1, axis=0)\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    The following 3 methods (setup_adam(), get_param_list(), and step_adam())\n",
        "    were generated by ChatGPT to implement the Adam optimizer into my model.\n",
        "    Originally I was using simple SGD but found convergence to take a long time\n",
        "    and that the training was very sensitive to the LR and initialized weights.\n",
        "    \"\"\"\n",
        "    def setup_adam(self, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        self.adam_lr = self.lr\n",
        "        self.adam_b1 = beta1\n",
        "        self.adam_b2 = beta2\n",
        "        self.adam_eps = eps\n",
        "        self.adam_t  = 0\n",
        "        self._m = {}   # first moments\n",
        "        self._v = {}   # second moments\n",
        "\n",
        "    def get_param_list(self):\n",
        "        return [\n",
        "            (\"W3_dec\", self.W3_dec, self.W3_dec_d), (\"b3_dec\", self.b3_dec, self.b3_dec_d),\n",
        "            (\"W2_dec\", self.W2_dec, self.W2_dec_d), (\"b2_dec\", self.b2_dec, self.b2_dec_d),\n",
        "            (\"W1_dec\", self.W1_dec, self.W1_dec_d), (\"b1_dec\", self.b1_dec, self.b1_dec_d),\n",
        "            (\"Wmu\",    self.Wmu,    self.Wmu_d),    (\"bmu\",    self.bmu,    self.bmu_d),\n",
        "            (\"Wstd\",   self.Wstd,   self.Wstd_d),   (\"bstd\",   self.bstd,   self.bstd_d),\n",
        "            (\"W2_enc\", self.W2_enc, self.W2_enc_d), (\"b2_enc\", self.b2_enc, self.b2_enc_d),\n",
        "            (\"W1_enc\", self.W1_enc, self.W1_enc_d), (\"b1_enc\", self.b1_enc, self.b1_enc_d),\n",
        "        ]\n",
        "\n",
        "    def step_adam(self):\n",
        "        self.adam_t += 1\n",
        "        b1, b2, eps = self.adam_b1, self.adam_b2, self.adam_eps\n",
        "        lr_t = self.adam_lr * ( (1 - b2**self.adam_t)**0.5 / (1 - b1**self.adam_t) )\n",
        "\n",
        "        for name, P, grad in self.get_param_list():\n",
        "\n",
        "            if name not in self._m:\n",
        "                # initialize moments to 0\n",
        "                self._m[name] = np.zeros_like(P)\n",
        "                self._v[name] = np.zeros_like(P)\n",
        "\n",
        "            m = self._m[name]\n",
        "            v = self._v[name]\n",
        "\n",
        "            # moment updates\n",
        "            m[...] = b1*m + (1-b1)*grad\n",
        "            v[...] = b2*v + (1-b2)*(grad*grad)\n",
        "\n",
        "            # optimizer step\n",
        "            P[...] -= lr_t * ( m / (np.sqrt(v) + eps) )\n",
        "\n",
        "    def set_beta(self, beta: float):\n",
        "        self.beta = float(beta)\n",
        "\n",
        "    def kl_beta_cycle(self, t, T, num_cycs, beta_max=1.0, ratio=0.5):\n",
        "        \"\"\"\n",
        "        Cyclical KL annealing from Fu et. al (2019).\n",
        "        \"\"\"\n",
        "        steps_per_cycle = T // num_cycs             # number of times params are changed in one cycle\n",
        "        cycle_pos = t % steps_per_cycle             # position within current cycle\n",
        "        warmup_steps = int(ratio * steps_per_cycle)\n",
        "        if cycle_pos < warmup_steps:\n",
        "            # linearly increase beta from 0 to beta_max over warmup_steps\n",
        "            return beta_max * (cycle_pos / warmup_steps)\n",
        "        else:\n",
        "          return beta_max\n",
        "\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        train_ds: Dataset,\n",
        "        test_ds: Dataset,\n",
        "        epochs:int,\n",
        "        batch_size:int,\n",
        "        beta_max:int,\n",
        "        idxs_to_visualize:List[int],\n",
        "        viz_dir:str,\n",
        "        nrow:int,\n",
        "        ncol:int,\n",
        "    ):\n",
        "\n",
        "        self.train_ds = train_ds\n",
        "        self.test_ds = test_ds\n",
        "        self.batch_size = batch_size\n",
        "        self.beta_max = beta_max\n",
        "        self.idxs_to_visualize = idxs_to_visualize\n",
        "        self.viz_dir = viz_dir\n",
        "        self.nrow = nrow\n",
        "        self.ncol = ncol\n",
        "\n",
        "        steps_per_epoch = int(np.ceil(len(train_ds) / self.batch_size))\n",
        "        total_trn_steps = epochs * steps_per_epoch\n",
        "\n",
        "        train_loader = DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True)\n",
        "        test_loader  = DataLoader(self.test_ds, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        trn_step = 0\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            train_rl, train_kl, train_elbo = 0, 0, 0\n",
        "            test_rl,  test_kl, test_elbo  = 0, 0, 0\n",
        "\n",
        "            for i, (batch, lbls) in enumerate(train_loader):\n",
        "                batch = batch.reshape(batch.shape[0], -1).numpy()  # vae class is not implemented for PyTorch tensors\n",
        "                # compute new beta\n",
        "                beta = self.kl_beta_cycle(\n",
        "                    t = trn_step,\n",
        "                    T = total_trn_steps,\n",
        "                    num_cycs = 4,\n",
        "                    beta_max=self.beta_max,\n",
        "                    ratio=0.25\n",
        "                )\n",
        "                self.set_beta(beta)\n",
        "                # forward pass\n",
        "                mu_x, mu, logvar = self.forward(batch)\n",
        "                # compute loss and accum\n",
        "                recon_loss, kl_div, elbo_loss = self.loss(batch, mu_x, logvar, mu)\n",
        "                train_rl += float(recon_loss)\n",
        "                train_kl += float(kl_div)\n",
        "                train_elbo += float(elbo_loss)\n",
        "                # backprop and optimization\n",
        "                self.backwards(batch, mu_x, logvar, mu)\n",
        "                self.step_adam()\n",
        "                trn_step += 1\n",
        "                if i%20==19:\n",
        "                    print(f\"Batch {i+1} | Recon Loss: {round(recon_loss, 4)} -------- KL Div: {round(kl_div, 4)}\")\n",
        "\n",
        "            # evaluate on test set\n",
        "            for i, (batch, lbls) in enumerate(test_loader):\n",
        "                batch = batch.reshape(batch.shape[0], -1).numpy()\n",
        "                mu_x, mu, logvar = self.forward(batch)\n",
        "                recon_loss, kl_div, elbo_loss = self.loss(batch, mu_x, logvar, mu)\n",
        "                test_rl += float(recon_loss)\n",
        "                test_kl += float(kl_div)\n",
        "                test_elbo += float(elbo_loss)\n",
        "\n",
        "            # visualize and save reconstructions of images in idxs_to_visualize\n",
        "            if idxs_to_visualize is not None:\n",
        "                  batch = np.vstack([test_ds[idx][0].flatten() for idx in idxs_to_visualize])\n",
        "                  mu_x, mu, logvar = self.forward(batch)\n",
        "                  mu_x = self.sigmoid(mu_x)\n",
        "                  arr = mu_x.reshape(-1, 28, 28)*255.0\n",
        "                  fig, axs = plt.subplots(nrow, ncol, figsize=(10, 10))\n",
        "                  flt_axs = axs.flatten()\n",
        "                  for i in range(len(idxs_to_visualize)):\n",
        "                      flt_axs[i].imshow(arr[i].reshape(28, 28), cmap='gray')\n",
        "                      flt_axs[i].axis('off')\n",
        "                      flt_axs[i].set_xticks([])\n",
        "                      flt_axs[i].set_yticks([])\n",
        "                  fig.tight_layout()\n",
        "                  fig.savefig(\n",
        "                      os.path.join(self.viz_dir, f\"tst_visualizations_epoch{epoch+1}.jpg\"),\n",
        "                      dpi=300,\n",
        "                      bbox_inches='tight'\n",
        "                  )\n",
        "\n",
        "            # average losses\n",
        "            test_rl /= len(test_loader)\n",
        "            test_kl /= len(test_loader)\n",
        "            test_elbo /= len(test_loader)\n",
        "            train_rl /= len(train_loader)\n",
        "            train_kl /= len(train_loader)\n",
        "            train_elbo /= len(train_loader)\n",
        "            self.test_losses[\"Recon Loss\"].append(test_rl)\n",
        "            self.test_losses[\"KL Loss\"].append(test_kl)\n",
        "            self.test_losses[\"ELBO Loss\"].append(test_elbo)\n",
        "            self.train_losses[\"Recon Loss\"].append(train_rl)\n",
        "            self.train_losses[\"KL Loss\"].append(train_kl)\n",
        "            self.train_losses[\"ELBO Loss\"].append(train_elbo)\n",
        "\n",
        "            # output\n",
        "            print(f\"EPOCH {epoch+1}\")\n",
        "            print(f\"Train Recon Loss: {round(train_rl, 4)} -------- Train KL Div: {round(train_kl, 4)}\")\n",
        "            print(f\"Test Recon Loss: {round(test_rl, 4)} -------- Test KL Div: {round(test_kl, 4)}\")\n",
        "            print('-'*60)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def generate_from_prior(self, n, scale=1, img_shape=(28, 28)):\n",
        "\n",
        "        # draw samples from gaussian prior for latent Z\n",
        "        z = scale * np.random.randn(n, self.latent_dim)\n",
        "        # pass Z through decoder to generate images\n",
        "        out = self.decoder(z)\n",
        "        # pass through sigmoid to get values in [0, 1]\n",
        "        imgs = 1/(1+np.exp(-out))\n",
        "        imgs = imgs.reshape(n, *img_shape)\n",
        "\n",
        "        k = int(np.ceil(np.sqrt(n)))\n",
        "        fig, axs = plt.subplots(k, k, figsize=(6,6))\n",
        "        for i, ax in enumerate(axs.flat):\n",
        "            if i < len(imgs):\n",
        "                ax.imshow(imgs[i], cmap=\"gray\")\n",
        "            ax.axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def save_model_state(self, path:str):\n",
        "        \"\"\"\n",
        "        Saves all model state information needed to resume training into a JSON file\n",
        "        \"\"\"\n",
        "\n",
        "        trn_state = {\n",
        "            \"l1_dim\":             int(self.l1_dim),\n",
        "            \"l2_dim\":             int(self.l2_dim),\n",
        "            \"latent_dim\" :        int(self.latent_dim),\n",
        "            \"l4_dim\":             int(self.l4_dim),\n",
        "            \"l5_dim\":             int(self.l5_dim),\n",
        "            \"mse\": self.mse,\n",
        "            \"trn_idxs\":           [int(x) for x in self.train_ds.indices],\n",
        "            \"tst_idxs\":           [int(x) for x in self.test_ds.indices],\n",
        "            \"train_losses\": {\n",
        "                \"Recon Loss\":     [float(x) for x in self.train_losses[\"Recon Loss\"]],\n",
        "                \"KL Loss\":        [float(x) for x in self.train_losses[\"KL Loss\"]],\n",
        "                \"ELBO Loss\":      [float(x) for x in self.train_losses[\"ELBO Loss\"]],\n",
        "            },\n",
        "            \"test_losses\": {\n",
        "                \"Recon Loss\":     [float(x) for x in self.test_losses[\"Recon Loss\"]],\n",
        "                \"KL Loss\":        [float(x) for x in self.test_losses[\"KL Loss\"]],\n",
        "                \"ELBO Loss\":      [float(x) for x in self.test_losses[\"ELBO Loss\"]],\n",
        "            },\n",
        "            \"beta_max\":           float(self.beta_max),\n",
        "            \"batch_size\":         int(self.batch_size),\n",
        "            \"lr\":                 float(self.lr),\n",
        "            \"beta\":               float(self.beta),\n",
        "            \"idxs_to_visualize\":  [int(x) for x in self.idxs_to_visualize],\n",
        "            \"viz_dir\":            self.viz_dir,\n",
        "            \"nrow\":               int(self.nrow),\n",
        "            \"ncol\":               int(self.ncol),\n",
        "            \"m\":                  {k: v.tolist() for k, v in self._m.items()},\n",
        "            \"v\":                  {k: v.tolist() for k, v in self._v.items()},\n",
        "            \"adam_t\":             float(self.adam_t),\n",
        "            \"adam_b1\":            float(self.adam_b1),\n",
        "            \"adam_b2\":            float(self.adam_b2),\n",
        "            \"adam_eps\":           float(self.adam_eps),\n",
        "            \"weights\": {\n",
        "              \"W1_enc\": self.W1_enc.tolist(),\n",
        "              \"b1_enc\": self.b1_enc.tolist(),\n",
        "              \"W2_enc\": self.W2_enc.tolist(),\n",
        "              \"b2_enc\": self.b2_enc.tolist(),\n",
        "              \"Wmu\": self.Wmu.tolist(),\n",
        "              \"bmu\": self.bmu.tolist(),\n",
        "              \"Wstd\": self.Wstd.tolist(),\n",
        "              \"bstd\": self.bstd.tolist(),\n",
        "              \"W1_dec\": self.W1_dec.tolist(),\n",
        "              \"b1_dec\": self.b1_dec.tolist(),\n",
        "              \"W2_dec\": self.W2_dec.tolist(),\n",
        "              \"b2_dec\": self.b2_dec.tolist(),\n",
        "              \"W3_dec\": self.W3_dec.tolist(),\n",
        "              \"b3_dec\": self.b3_dec.tolist()\n",
        "          }\n",
        "        }\n",
        "\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(trn_state, f)\n",
        "        print(\"Success! Training information saved to:\", path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PZiQGv_wgVdW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZiQGv_wgVdW",
        "outputId": "b9f04c0a-2b65-4c3b-c397-ad9aa7b2123b"
      },
      "outputs": [],
      "source": [
        "beta_max = 10\n",
        "os.makedirs(f\"/content/drive/MyDrive/VariationalAutoEncoders/imgs/VanillaVAE/beta={beta_max}\", exist_ok=True)\n",
        "\n",
        "# split into testing and training splits\n",
        "test_split = 0.2\n",
        "test_size = int(test_split * len(mnist_dataset))\n",
        "train_size  = len(mnist_dataset) - test_size\n",
        "\n",
        "# creates Subset objects from orig. Dataset\n",
        "train_ds, test_ds = random_split(mnist_dataset, [train_size, test_size])\n",
        "\n",
        "# choose random images to visualze during training from test set\n",
        "idxs = list(np.random.choice(np.arange(len(test_ds)), 12))\n",
        "\n",
        "vae = VariationalAutoencoder(\n",
        "    lr=0.0005,\n",
        "    l1_dim=256,\n",
        "    l2_dim=64,\n",
        "    latent_dim=16,\n",
        "    l4_dim=64,\n",
        "    l5_dim=256,\n",
        "    mse=False\n",
        ")\n",
        "\n",
        "vae.setup_adam()  # initializes moments\n",
        "\n",
        "vae.train(\n",
        "    train_ds,\n",
        "    test_ds,\n",
        "    epochs=25,\n",
        "    batch_size=64,\n",
        "    beta_max=beta_max,\n",
        "    idxs_to_visualize=idxs,\n",
        "    viz_dir = f\"/content/drive/MyDrive/VariationalAutoEncoders/imgs/VanillaVAE/beta={beta_max}\",\n",
        "    nrow=3,\n",
        "    ncol=4,\n",
        ")\n",
        "\n",
        "save_state_path = \"/content/drive/MyDrive/VariationalAutoEncoders/models/VanillaVAE\"\n",
        "vae.save_model_state(os.path.join(save_state_path, f\"linear_vae_params_{date}_beta={beta_max}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8qGWlbKSuTKq",
      "metadata": {
        "id": "8qGWlbKSuTKq"
      },
      "source": [
        "### Count number of parameters in the VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VVfXD23guMG9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVfXD23guMG9",
        "outputId": "c3354052-5ba9-4016-a23a-45f14cd039c4"
      },
      "outputs": [],
      "source": [
        "ps = 0\n",
        "for w in vae.get_param_list():\n",
        "  shp = w[1].shape\n",
        "  if len(shp)==2:\n",
        "      ps += shp[0]*shp[1]\n",
        "  else:\n",
        "    ps += shp[0]\n",
        "print(\"Number of trainable parameters in the model:\", ps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_2c2t_TVF6Ku",
      "metadata": {
        "id": "_2c2t_TVF6Ku"
      },
      "outputs": [],
      "source": [
        "plt.style.use(\"bmh\")\n",
        "\n",
        "def plot_losses(vae):\n",
        "\n",
        "    train_rl_loss = vae.train_losses[\"Recon Loss\"]\n",
        "    test_rl_loss = vae.test_losses[\"Recon Loss\"]\n",
        "    train_kl_loss = vae.train_losses[\"KL Loss\"]\n",
        "    test_kl_loss = vae.test_losses[\"KL Loss\"]\n",
        "    train_elbo_loss = vae.train_losses[\"ELBO Loss\"]\n",
        "    test_elbo_loss = vae.test_losses[\"ELBO Loss\"]\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    flat_axs = axs.flatten()\n",
        "\n",
        "    flat_axs[0].plot(train_rl_loss, label=\"Train\")\n",
        "    flat_axs[0].plot(test_rl_loss, label=\"Test\")\n",
        "    flat_axs[0].set_title(\"Reconstruction Loss\")\n",
        "    flat_axs[0].set_xlabel(\"Epoch\")\n",
        "    flat_axs[0].legend()\n",
        "    #   flat_axs[1].yaxis.set_major_formatter(plt.FormatStrFormatter('%.3f'))\n",
        "\n",
        "    flat_axs[1].plot(train_kl_loss, label=\"Train\")\n",
        "    flat_axs[1].plot(test_kl_loss, label=\"Test\")\n",
        "    flat_axs[1].set_title(\"KL Divergence\")\n",
        "    flat_axs[1].set_xlabel(\"Epoch\")\n",
        "    #   flat_axs[1].yaxis.set_major_formatter(plt.FormatStrFormatter('%.3f'))\n",
        "\n",
        "    flat_axs[2].plot(train_elbo_loss, label=\"Train\")\n",
        "    flat_axs[2].plot(test_elbo_loss, label=\"Test\")\n",
        "    flat_axs[2].set_title(\"Negative ELBO\")\n",
        "    flat_axs[2].set_xlabel(\"Epoch\")\n",
        "    #   flat_axs[2].yaxis.set_major_formatter(plt.FormatStrFormatter('%.3f'))\n",
        "\n",
        "    fig.suptitle(f\"VAE Losses Beta_max = {vae.beta_max}\", y=1.03)\n",
        "\n",
        "    return fig, axs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NJ0WkyZLM6pk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "NJ0WkyZLM6pk",
        "outputId": "17309e18-2d5a-4bd1-91ec-11f2f2db17a0"
      },
      "outputs": [],
      "source": [
        "fig, axs = plot_losses(vae)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "x8Lr-strxr83",
      "metadata": {
        "id": "x8Lr-strxr83"
      },
      "outputs": [],
      "source": [
        "def resume_training_vae(trn_state_path: str, dataset: Dataset, epochs: int) -> VariationalAutoencoder:\n",
        "\n",
        "    \"\"\"\n",
        "    Loads a model from a checkpoint file and continues training. Pass epochs=0\n",
        "    to get a copy of the model at the end of training.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(trn_state_path, 'r') as f:\n",
        "        trn_state = json.load(f)\n",
        "\n",
        "    vae = VariationalAutoencoder(\n",
        "        lr=trn_state[\"lr\"],\n",
        "        l1_dim=trn_state[\"l1_dim\"],\n",
        "        l2_dim=trn_state[\"l2_dim\"],\n",
        "        latent_dim=trn_state[\"latent_dim\"],\n",
        "        l4_dim=trn_state[\"l4_dim\"],\n",
        "        l5_dim=trn_state[\"l5_dim\"],\n",
        "        mse=trn_state[\"mse\"]\n",
        "    )\n",
        "\n",
        "    vae.W1_enc = np.array(trn_state[\"weights\"][\"W1_enc\"])\n",
        "    vae.b1_enc = np.array(trn_state[\"weights\"][\"b1_enc\"])\n",
        "    vae.W2_enc = np.array(trn_state[\"weights\"][\"W2_enc\"])\n",
        "    vae.b2_enc = np.array(trn_state[\"weights\"][\"b2_enc\"])\n",
        "    vae.Wmu = np.array(trn_state[\"weights\"][\"Wmu\"])\n",
        "    vae.bmu = np.array(trn_state[\"weights\"][\"bmu\"])\n",
        "    vae.Wstd = np.array(trn_state[\"weights\"][\"Wstd\"])\n",
        "    vae.bstd = np.array(trn_state[\"weights\"][\"bstd\"])\n",
        "    vae.W1_dec = np.array(trn_state[\"weights\"][\"W1_dec\"])\n",
        "    vae.b1_dec = np.array(trn_state[\"weights\"][\"b1_dec\"])\n",
        "    vae.W2_dec = np.array(trn_state[\"weights\"][\"W2_dec\"])\n",
        "    vae.b2_dec = np.array(trn_state[\"weights\"][\"b2_dec\"])\n",
        "    vae.W3_dec = np.array(trn_state[\"weights\"][\"W3_dec\"])\n",
        "    vae.b3_dec = np.array(trn_state[\"weights\"][\"b3_dec\"])\n",
        "\n",
        "    vae._m = {k: np.array(trn_state[\"m\"][k]) for k in trn_state[\"m\"].keys()}\n",
        "    vae._v = {k: np.array(trn_state[\"v\"][k]) for k in trn_state[\"v\"].keys()}\n",
        "    vae.adam_t = trn_state[\"adam_t\"]\n",
        "    vae.adam_b1 = trn_state[\"adam_b1\"]\n",
        "    vae.adam_b2 = trn_state[\"adam_b2\"]\n",
        "    vae.adam_eps = trn_state[\"adam_eps\"]\n",
        "    vae.adam_lr = trn_state[\"lr\"]\n",
        "\n",
        "    vae.train_losses = trn_state[\"train_losses\"]\n",
        "    vae.test_losses = trn_state[\"test_losses\"]\n",
        "\n",
        "    train_ds = Subset(dataset, trn_state[\"trn_idxs\"])\n",
        "    test_ds = Subset(dataset, trn_state[\"tst_idxs\"])\n",
        "\n",
        "    vae.params_init = True\n",
        "\n",
        "    if epochs!=0:\n",
        "      vae.train(\n",
        "          train_ds = train_ds,\n",
        "          test_ds = test_ds,\n",
        "          epochs = epochs,\n",
        "          batch_size = trn_state[\"batch_size\"],\n",
        "          beta_max = trn_state[\"beta_max\"],\n",
        "          warmup_epochs = trn_state[\"wmp_epochs\"],\n",
        "          idxs_to_visualize = trn_state[\"idxs_to_visualize\"],\n",
        "          viz_dir = trn_state[\"viz_dir\"],\n",
        "          nrow = trn_state[\"nrow\"],\n",
        "          ncol = trn_state[\"ncol\"]\n",
        "      )\n",
        "      vae.save_model_state(trn_state_path)\n",
        "\n",
        "    return vae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2WUBaVKhNTS_",
      "metadata": {
        "id": "2WUBaVKhNTS_"
      },
      "outputs": [],
      "source": [
        "vae = resume_training_vae(\n",
        "    trn_state_path=\"/content/drive/MyDrive/VariationalAutoEncoders/models/VanillaVAE/linear_vae_params_2025-11-28-192256_beta=10\",\n",
        "    dataset=mnist_dataset,\n",
        "    epochs=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "au_sM3yE9S3q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "au_sM3yE9S3q",
        "outputId": "f437653d-2b34-4af5-b1aa-7d1e6f02f003"
      },
      "outputs": [],
      "source": [
        "vae.generate_from_prior(n=16, scale=1, img_shape=(28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y0DYzmPmF2E0",
      "metadata": {
        "id": "Y0DYzmPmF2E0"
      },
      "source": [
        "## Building the Convolutional Variational Auto encoder (CVAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7ab8615",
      "metadata": {
        "id": "e7ab8615"
      },
      "outputs": [],
      "source": [
        "class CBVAEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim: int, beta_max=1.0) -> None:\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.beta_max = beta_max\n",
        "        self.beta = beta_max\n",
        "        self.train_losses = {\"Recon Loss\": [], \"KL Loss\": [], \"ELBO Loss\": []}\n",
        "        self.test_losses  = {\"Recon Loss\": [], \"KL Loss\": [], \"ELBO Loss\": []}\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            # Bx3x64x64 -> Bx32x32x32\n",
        "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Bx32x32x32 -> Bx64x16x16\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Bx64x16x16 -> Bx128x8x8\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Bx128x8x8 -> Bx256x4x4\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Bx(256*4*4)\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(256*4*4, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, latent_dim * 2)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256*4*4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Bx256x4x4\n",
        "            nn.Unflatten(1, (256, 4, 4)),\n",
        "            # 4x4 -> 8x8\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # 8x8 -> 16x16\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # 16x16 -> 32x32\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # 32x32 -> 64x64\n",
        "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
        "            # map outputs in [0,1]\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def set_beta(self, beta: float):\n",
        "        self.beta = beta\n",
        "\n",
        "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, X):\n",
        "        enc_out = self.encoder(X)\n",
        "        mu_phi, logvar_phi = torch.split(enc_out, self.latent_dim, dim=1)\n",
        "        z = self.reparameterize(mu_phi, logvar_phi)\n",
        "        Mu_x = self.decoder(z)\n",
        "        return Mu_x, mu_phi, logvar_phi\n",
        "\n",
        "\n",
        "    def kl_beta_cycle(self, t, T, num_cycs, beta_max=1.0, ratio=0.5):\n",
        "        \"\"\"\n",
        "        Cyclical KL annealing from Fu et. al (2019).\n",
        "        \"\"\"\n",
        "        steps_per_cycle = T // num_cycs             # number of times params are changed in one cycle\n",
        "        cycle_pos = t % steps_per_cycle             # position within current cycle\n",
        "        warmup_steps = int(ratio * steps_per_cycle)\n",
        "        if cycle_pos < warmup_steps:\n",
        "            # linearly increase beta from 0 to beta_max over warmup_steps\n",
        "            return beta_max * (cycle_pos / warmup_steps)\n",
        "        else:\n",
        "          return beta_max\n",
        "\n",
        "    def generate_from_prior(self, n, device, scale=1, img_shape=(3, 64, 64)):\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # draw samples from gaussian prior for latent Z\n",
        "          z = scale * np.random.randn(n, self.latent_dim)\n",
        "          z = torch.from_numpy(z).to(torch.float32).to(device)\n",
        "          # pass Z through decoder to generate images\n",
        "          imgs = self.decoder(z)\n",
        "          imgs = imgs.reshape(n, *img_shape)\n",
        "          # imshow expects color channels to come last\n",
        "          imgs = imgs.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
        "          k = int(np.ceil(np.sqrt(n)))\n",
        "          fig, axs = plt.subplots(k, k, figsize=(6,6))\n",
        "          for i, ax in enumerate(axs.flat):\n",
        "              if i < len(imgs):\n",
        "                  ax.imshow(imgs[i])\n",
        "              ax.axis(\"off\")\n",
        "          plt.tight_layout()\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "def loss(X, Mu_x, mu_phi, logvar_phi, recon_loss_fn, beta):\n",
        "    # reconstruction loss\n",
        "    recon_loss = recon_loss_fn(Mu_x, X)\n",
        "    # KL divergence loss\n",
        "    kl_div = (-0.5 * torch.mean(\n",
        "        1 + logvar_phi - mu_phi.pow(2) - logvar_phi.exp(),\n",
        "        dim=1\n",
        "    )).mean()\n",
        "    elbo = recon_loss + beta * kl_div\n",
        "    return recon_loss, kl_div, elbo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jbnq2WM5MtCd",
      "metadata": {
        "id": "jbnq2WM5MtCd"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    optimizer: torch.optim,\n",
        "    model: CBVAEncoder,\n",
        "    train_ds: Dataset,\n",
        "    test_ds: Dataset,\n",
        "    epochs:int,\n",
        "    batch_size:int,\n",
        "    idxs_to_visualize:List[int],\n",
        "    viz_dir:str,\n",
        "    nrow:int,\n",
        "    ncol:int,\n",
        "    device: torch.cuda.device\n",
        "):\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "    steps_per_epoch = int(np.ceil(len(train_ds) / batch_size))\n",
        "    total_trn_steps = epochs * steps_per_epoch\n",
        "    trn_step = 0  # keeps track of number of param updates for KL annealing\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # init per-epoch running losses\n",
        "        train_rl, train_kl, train_elbo = 0, 0, 0\n",
        "        test_rl,  test_kl, test_elbo  = 0, 0, 0\n",
        "\n",
        "        # training loop\n",
        "        model.train()\n",
        "        for i, (batch, lbls) in enumerate(train_loader):\n",
        "            batch = batch.to(device)\n",
        "            # clear grads\n",
        "            optimizer.zero_grad()\n",
        "            # compute new beta\n",
        "            # beta = model.kl_beta_cycle(\n",
        "            #     t = trn_step,\n",
        "            #     T = total_trn_steps,\n",
        "            #     num_cycs = 8,\n",
        "            #     beta_max=model.beta_max,\n",
        "            #     ratio=0.6\n",
        "            # )\n",
        "            model.set_beta(beta_max)\n",
        "            # forward pass\n",
        "            mu_x, mu, logvar = model(batch)\n",
        "            # compute loss and accum\n",
        "            recon_loss, kl_div, elbo = loss(batch, mu_x, mu, logvar, recon_loss_fn=nn.BCELoss(reduction=\"mean\"), beta=model.beta)\n",
        "            train_rl += float(recon_loss.detach())\n",
        "            train_kl += float(kl_div.detach())\n",
        "            train_elbo += float(elbo.detach())\n",
        "            # backprop and optimization\n",
        "            elbo.backward()\n",
        "            optimizer.step()\n",
        "            trn_step += 1\n",
        "\n",
        "            # display batch loss info\n",
        "            if i%10==9:\n",
        "                print(f\"Batch {i+1} | Recon Loss: {torch.round(recon_loss, decimals=4)} -------- KL Div: {torch.round(kl_div, decimals=4)}\")\n",
        "\n",
        "        # testing loop\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (batch, lbls) in enumerate(test_loader):\n",
        "                batch = batch.to(device)\n",
        "                mu_x, mu, logvar = model(batch)\n",
        "                recon_loss, kl_div, elbo = loss(batch, mu_x, mu, logvar, recon_loss_fn=nn.BCELoss(reduction=\"mean\"), beta=model.beta)\n",
        "                test_rl += float(recon_loss.detach())\n",
        "                test_kl += float(kl_div.detach())\n",
        "                test_elbo += float(elbo.detach())\n",
        "\n",
        "            # visualize and save reconstructions of images in idxs_to_visualize\n",
        "            if idxs_to_visualize is not None:\n",
        "                  batch = torch.stack([test_ds[idx][0] for idx in idxs_to_visualize], dim=0)\n",
        "                  batch = batch.to(device)\n",
        "                  mu_x, mu, logvar = model(batch)\n",
        "                  fig, axs = plt.subplots(nrow, ncol, figsize=(10, 10))\n",
        "                  flt_axs = axs.flatten()\n",
        "                  for i in range(len(idxs_to_visualize)):\n",
        "                      flt_axs[i].imshow(mu_x[i].cpu().permute(1, 2, 0).numpy())\n",
        "                      flt_axs[i].axis('off')\n",
        "                      flt_axs[i].set_xticks([])\n",
        "                      flt_axs[i].set_yticks([])\n",
        "                  fig.tight_layout()\n",
        "                  fig.savefig(\n",
        "                      os.path.join(viz_dir, f\"tst_visualizations_epoch{epoch+1}.jpg\"),\n",
        "                      dpi=300,\n",
        "                      bbox_inches='tight'\n",
        "                  )\n",
        "\n",
        "        # loss updates\n",
        "        test_rl /= len(test_loader)\n",
        "        test_kl /= len(test_loader)\n",
        "        test_elbo /= len(test_loader)\n",
        "        train_rl /= len(train_loader)\n",
        "        train_kl /= len(train_loader)\n",
        "        train_elbo /= len(train_loader)\n",
        "        model.test_losses[\"Recon Loss\"].append(test_rl)\n",
        "        model.test_losses[\"KL Loss\"].append(test_kl)\n",
        "        model.test_losses[\"ELBO Loss\"].append(test_elbo)\n",
        "        model.train_losses[\"Recon Loss\"].append(train_rl)\n",
        "        model.train_losses[\"KL Loss\"].append(train_kl)\n",
        "        model.train_losses[\"ELBO Loss\"].append(train_elbo)\n",
        "\n",
        "        # output\n",
        "        print(f\"EPOCH {epoch+1}\")\n",
        "        print(f\"Train Recon Loss: {round(train_rl, 4)} ------ Train KL Div: {round(train_kl, 4)} ------ Train ELBO: {round(train_elbo, 4)}\")\n",
        "        print(f\"Test Recon Loss: {round(test_rl, 4)} ------ Test KL Div: {round(test_kl, 4)} ------ Test ELBO: {round(test_elbo, 4)}\")\n",
        "        print('-'*60)\n",
        "\n",
        "    return model, optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5231367",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5231367",
        "outputId": "76ee1e8a-35ba-4a16-9361-90df4954ac99"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/VariationalAutoEncoders/models/CVAE/cvae_checkpoint.pth\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# training vars\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "lr = 0.001\n",
        "beta_max = 1\n",
        "latent_dim = 256\n",
        "checkpoint_path = f\"/content/drive/MyDrive/VariationalAutoEncoders/models/CVAE/cvae_checkpoint_beta={beta_max}.pth\"\n",
        "\n",
        "# visualization vars\n",
        "num_to_vis = 12\n",
        "nrow, ncol = 3, 4\n",
        "viz_dir = f\"/content/drive/MyDrive/VariationalAutoEncoders/imgs/CVAE/beta={beta_max}\"\n",
        "os.makedirs(viz_dir, exist_ok=True)\n",
        "\n",
        "cvae = CBVAEncoder(latent_dim=latent_dim, beta_max=beta_max)\n",
        "cvae.to(device)\n",
        "optimizer = optim.Adam(cvae.parameters(), lr=lr)\n",
        "\n",
        "# split data into testing and training splits\n",
        "test_split = 0.1\n",
        "test_size = int(test_split * len(lfw_dataset))\n",
        "train_size  = len(lfw_dataset) - test_size\n",
        "\n",
        "# creates Subset objects from orig. Dataset\n",
        "train_ds, test_ds = random_split(lfw_dataset, [train_size, test_size])\n",
        "train_idxs, test_idxs = train_ds.indices, test_ds.indices\n",
        "\n",
        "# choose random images to visualze during training from test set\n",
        "idxs = list(np.random.choice(np.arange(len(test_ds)), num_to_vis))\n",
        "\n",
        "cvae, optimizer = train(\n",
        "    optimizer = optimizer,\n",
        "    model = cvae,\n",
        "    train_ds = train_ds,\n",
        "    test_ds = test_ds,\n",
        "    epochs = epochs,\n",
        "    batch_size = batch_size,\n",
        "    idxs_to_visualize = idxs,\n",
        "    viz_dir = viz_dir,\n",
        "    nrow = nrow,\n",
        "    ncol = ncol,\n",
        "    device = device\n",
        ")\n",
        "\n",
        "checkpoint = {\n",
        "    'model_state_dict': cvae.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'epochs': epochs,\n",
        "    'train_idxs': train_idxs,\n",
        "    'test_idxs': test_idxs,\n",
        "    'batch_size': batch_size,\n",
        "    'lr': lr,\n",
        "    'train_losses': cvae.train_losses,\n",
        "    'test_losses': cvae.test_losses,\n",
        "    'beta_max': cvae.beta_max,\n",
        "    'beta': cvae.beta,\n",
        "    'latent_dim': cvae.latent_dim,\n",
        "    'idxs_to_visualize': idxs,\n",
        "    'viz_dir': viz_dir,\n",
        "    'nrow': nrow,\n",
        "    'ncol': ncol\n",
        "}\n",
        "torch.save(checkpoint, checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7QyBCLz3Mwmb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QyBCLz3Mwmb",
        "outputId": "bed88eb5-4b3d-4abf-eeec-4ad4c492193f"
      },
      "outputs": [],
      "source": [
        "# count number of parameters in cvae\n",
        "num_params = sum(p.numel() for p in cvae.parameters() if p.requires_grad)\n",
        "print(\"Number of trainable parameters in model:\", num_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y9lxdrUTT7Z2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "Y9lxdrUTT7Z2",
        "outputId": "37820c05-b486-4abd-92bc-062f603dc82f"
      },
      "outputs": [],
      "source": [
        "cvae.generate_from_prior(n=20, device=device, scale=1, img_shape=(3, 64, 64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U22IAXviLbC9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "U22IAXviLbC9",
        "outputId": "0f28cba0-ab15-4af5-ca48-9fa6545b298b"
      },
      "outputs": [],
      "source": [
        "plot_losses(cvae)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0IMghQ4qFngG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0IMghQ4qFngG",
        "outputId": "112d574b-2d2e-4b87-d0be-d51e241ae815"
      },
      "outputs": [],
      "source": [
        "def resume_training_cvae(checkpoint_path: str, dataset: Dataset, epochs: int, device):\n",
        "\n",
        "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "\n",
        "    # model hparams\n",
        "    batch_size = ckpt[\"batch_size\"]\n",
        "    beta_max = ckpt[\"beta_max\"]\n",
        "    beta = ckpt[\"beta\"]\n",
        "    latent_dim = ckpt[\"latent_dim\"]\n",
        "\n",
        "    # create model and optimizer\n",
        "    cvae = CBVAEncoder(latent_dim, beta_max)\n",
        "    cvae.to(device)\n",
        "    cvae.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "    optimizer = optim.Adam(cvae.parameters(), lr=ckpt[\"lr\"])\n",
        "    optim_state = ckpt[\"optimizer_state_dict\"]\n",
        "    optimizer.load_state_dict(optim_state)\n",
        "\n",
        "    # load datasets and split into training and testing\n",
        "    train_ds = Subset(dataset, ckpt[\"train_idxs\"])\n",
        "    test_ds = Subset(dataset, ckpt[\"test_idxs\"])\n",
        "\n",
        "    # init per-epoch losses\n",
        "    cvae.train_losses = ckpt[\"train_losses\"]\n",
        "    cvae.test_losses = ckpt[\"test_losses\"]\n",
        "\n",
        "    print(ckpt[\"train_losses\"])\n",
        "\n",
        "    cvae.set_beta(ckpt[\"beta\"])\n",
        "\n",
        "    # per-epoch visualization configs\n",
        "    idxs_to_visualize = ckpt[\"idxs_to_visualize\"]\n",
        "    viz_dir = ckpt[\"viz_dir\"]\n",
        "    nrow = ckpt[\"nrow\"]\n",
        "    ncol = ckpt[\"ncol\"]\n",
        "\n",
        "    cvae, optimizer = train(\n",
        "        optimizer = optimizer,\n",
        "        model = cvae,\n",
        "        train_ds = train_ds,\n",
        "        test_ds = test_ds,\n",
        "        epochs = epochs,\n",
        "        batch_size = batch_size,\n",
        "        idxs_to_visualize = idxs_to_visualize,\n",
        "        viz_dir = viz_dir,\n",
        "        nrow = nrow,\n",
        "        ncol = ncol,\n",
        "        device = device\n",
        "    )\n",
        "\n",
        "    # saved trained model\n",
        "    checkpoint = {\n",
        "        'model_state_dict': cvae.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epochs': ckpt[\"epochs\"] + epochs,\n",
        "        'train_idxs': ckpt[\"train_idxs\"],\n",
        "        'test_idxs': ckpt[\"test_idxs\"],\n",
        "        'batch_size': batch_size,\n",
        "        'lr': ckpt[\"lr\"],\n",
        "        'train_losses': cvae.train_losses,\n",
        "        'test_losses': cvae.test_losses,\n",
        "        'beta_max': cvae.beta_max,\n",
        "        'beta': cvae.beta,\n",
        "        'latent_dim': cvae.latent_dim,\n",
        "        'idxs_to_visualize': idxs_to_visualize,\n",
        "        'viz_dir': viz_dir,\n",
        "        'nrow': nrow,\n",
        "        'ncol': ncol\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    return cvae, optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vOdaIzdLP7GA",
      "metadata": {
        "id": "vOdaIzdLP7GA"
      },
      "outputs": [],
      "source": [
        "conv_vae, opt = resume_training_cvae(\n",
        "    \"/content/drive/MyDrive/VariationalAutoEncoders/models/CVAE/cvae_checkpoint_beta=0.05.pth\",\n",
        "    dataset=lfw_dataset,\n",
        "    epochs=0,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "conv_vae.generate_from_prior(n=20, device=device, scale=1, img_shape=(3, 64, 64))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "areSf1-wTmdM",
      "metadata": {
        "id": "areSf1-wTmdM"
      },
      "source": [
        "## Run this code if you only just want to use the pretrained models to generate new images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ehqKRu23TrW3",
      "metadata": {
        "id": "ehqKRu23TrW3"
      },
      "source": [
        "### MNIST Handwritten Digit Image Generation\n",
        "\n",
        "Running this cell requires that the cell that downloads the MNIST dataset has been ran and that the cells that define VariationalAutoencoder class and the resume_training_vae() function have been ran."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4azKeSdkTlMk",
      "metadata": {
        "id": "4azKeSdkTlMk"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 16 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_path = os.path.join(\n",
        "    working_dir, \n",
        "    \"models\", \n",
        "    \"VanillaVAE\", \n",
        "    \"linear_vae_params_2025-12-07-001025_beta=5\"\n",
        "    )\n",
        "\n",
        "vae = resume_training_vae(\n",
        "    trn_state_path = model_path,\n",
        "    dataset = mnist_dataset,\n",
        "    epochs = 0\n",
        ")\n",
        "\n",
        "vae.generate_from_prior(n=16, scale=1, img_shape=(28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3EQPh3LVTvbA",
      "metadata": {
        "id": "3EQPh3LVTvbA"
      },
      "source": [
        "### LFW Human Faces Image Generation\n",
        "\n",
        "Running this cell requires that the cells that downloads the LFW dataset and create the Dataset class have been ran as well as the cells that define VariationalAutoencoder class and the resume_training_cvae()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RfuE5INcNyb3",
      "metadata": {
        "id": "RfuE5INcNyb3"
      },
      "outputs": [],
      "source": [
        "model_path = os.path.join(\n",
        "    working_dir, \n",
        "    \"models\", \n",
        "    \"CVAE\", \n",
        "    \"cvae_checkpoint_beta=0.01.pth\"\n",
        "    )\n",
        "\n",
        "\n",
        "cvae, optimizer = resume_training_cvae(\n",
        "    checkpoint_path = model_path,\n",
        "    dataset = LFW_dataset,\n",
        "    epochs = 0\n",
        ")\n",
        "\n",
        "cvae.generate_from_prior(n=20, device=device, scale=1, img_shape=(3, 64, 64))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
